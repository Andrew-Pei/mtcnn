{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T03:12:48.702318Z",
     "start_time": "2020-09-19T03:12:47.896791Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8,
     34,
     37,
     53,
     59,
     62,
     65,
     68,
     69,
     102,
     105
    ]
   },
   "outputs": [],
   "source": [
    "class RebalancedVideoDataset(Dataset):\n",
    "    \"\"\"This ensures that a every epoch classes are balanced without repetition, classes with more examples\n",
    "    use their full range of videos.\n",
    "    The requested id maps directly 1:1 with the smallest class, whereas larger classes get a video chosen\n",
    "    with the corresponding range. E.g. if class1 has videos 1:100, and class 2 has videos 101:500, then\n",
    "    requesting dataset[5] always results in the 5th video of class one, but dataset[105] will randomly yield\n",
    "    one of 5 videos in range 125 - 130.\"\"\"\n",
    "\n",
    "    def __init__(self, video_dir, train_or_test, label_per_frame, transforms, framewise_transforms, i3d_norm, test_videos=None,\n",
    "                 test_proportion=0.25, file_ext=\".mp4\", max_frames=64, bce_labels=False, alt_aug=False):\n",
    "        self.video_dir = video_dir\n",
    "        self.train_or_test = train_or_test\n",
    "        self.label_per_frame = label_per_frame\n",
    "        self.test_videos = test_videos\n",
    "        self.test_proportion = test_proportion\n",
    "        self.file_ext = file_ext\n",
    "        self.i3d_norm = i3d_norm\n",
    "        self.max_frames = max_frames\n",
    "        self.transforms = transforms\n",
    "        self.framewise_transforms = framewise_transforms\n",
    "        self.bce_labels = bce_labels\n",
    "        self.alt_aug = alt_aug\n",
    "        self.classes = self.get_classes()\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.videos_by_class = self.get_videos_by_class()\n",
    "        self.n_by_class = self.get_n_by_class()\n",
    "        self.n_smallest_class = self.get_n_smallest_class()\n",
    "        self.n_balanced = self.get_n_balanced()\n",
    "        self.n_unbalanced = self.get_n_unbalanced()\n",
    "\n",
    "        self.c = self.n_classes  # FastAI\n",
    "\n",
    "        self.summary()\n",
    "\n",
    "    def get_classes(self):\n",
    "        return os.listdir(self.video_dir)\n",
    "\n",
    "    def get_videos_by_class(self):\n",
    "        videos_by_class = {}\n",
    "        for cls in self.classes:\n",
    "            videos_for_class = []\n",
    "            videopaths = glob(os.path.join(self.video_dir, cls, f\"*{self.file_ext}\"))\n",
    "            for videopath in videopaths:\n",
    "                is_test = self.train_or_test == 'test'\n",
    "\n",
    "                video_chunk_id = os.path.basename(videopath).split('_', 1)[0]\n",
    "                in_test = video_chunk_id in self.test_videos\n",
    "                if is_test == in_test:\n",
    "                    videos_for_class.append(videopath)\n",
    "\n",
    "            videos_by_class[cls] = videos_for_class\n",
    "        return videos_by_class\n",
    "\n",
    "    def get_n_by_class(self):\n",
    "        n_by_class = {}\n",
    "        for cls, videos in self.videos_by_class.items():\n",
    "            n_by_class[cls] = len(videos)\n",
    "        return n_by_class\n",
    "\n",
    "    def get_n_smallest_class(self):\n",
    "        return min([len(videos) for videos in self.videos_by_class.values()])\n",
    "\n",
    "    def get_n_balanced(self):\n",
    "        return self.get_n_smallest_class() * self.n_classes\n",
    "\n",
    "    def get_n_unbalanced(self):\n",
    "        return sum([len(videos) for videos in self.videos_by_class.values()])\n",
    "\n",
    "    def summary(self):\n",
    "        print(f\"{self.train_or_test.upper()}:\"\n",
    "              f\"Loaded {self.n_unbalanced} samples across classes '{', '.join(self.classes)}'; effective sample size of {self.n_balanced}\")\n",
    "\n",
    "    def load_video(self, filename, every_n_frames=1, to_rgb, rescale=None):\n",
    "        cap = cv2.VideoCapture(filename)\n",
    "        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        if rescale:\n",
    "            out_video = np.zeros(\n",
    "                (math.ceil(frameCount / every_n_frames), int(frameHeight * rescale), int(frameWidth * rescale), 3),\n",
    "                np.dtype('uint8'))\n",
    "        else:\n",
    "            out_video = np.zeros((math.ceil(frameCount / every_n_frames), frameHeight, frameWidth, 3),\n",
    "                                 np.dtype('uint8'))\n",
    "\n",
    "        i_frame = 0\n",
    "        ret = True\n",
    "\n",
    "        while (i_frame * every_n_frames < frameCount and ret):\n",
    "            cap.set(cv2.CAP_PROP_FRAME_COUNT, (i_frame * every_n_frames) - 1)\n",
    "            ret, frame = cap.read()\n",
    "            if rescale:\n",
    "                frame = cv2.resize(frame, (0, 0), fx=rescale, fy=rescale)\n",
    "            if to_rgb:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            out_video[i_frame] = frame\n",
    "            i_frame += 1\n",
    "\n",
    "        cap.release()\n",
    "        return out_video\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_balanced\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the class\n",
    "        id_cls = idx // self.n_smallest_class\n",
    "        cls = self.classes[id_cls]\n",
    "\n",
    "        # Get the video within the class\n",
    "        n_cls = self.n_by_class[cls]\n",
    "        id_in_cls_bal = idx % self.n_smallest_class\n",
    "        id_in_cls_from = math.ceil((id_in_cls_bal / self.n_smallest_class) * n_cls)\n",
    "        id_in_cls_to = max(id_in_cls_from,\n",
    "                           math.floor((((\n",
    "                                                    id_in_cls_bal + 1) / self.n_smallest_class) * n_cls) - 0.0001))  # Small epsilon to make sure whole numbers round down (so math.ceil != math.floor)\n",
    "        id_in_cls = random.randint(id_in_cls_from, id_in_cls_to)\n",
    "\n",
    "        # Load the video\n",
    "        videoname = self.videos_by_class[cls][id_in_cls]\n",
    "        video = self.load_video(filename=videoname, every_n_frames=1, to_rgb=True)\n",
    "\n",
    "        if self.alt_aug:\n",
    "            frame_incrementer = random.randint(1, 2)  # 1 for no aug, 2 for 1\n",
    "        else:\n",
    "            frame_incrementer = 1\n",
    "        max_frames = self.max_frames * frame_incrementer\n",
    "\n",
    "        if self.train_or_test == 'test':\n",
    "            starting_frame = 0\n",
    "        elif self.train_or_test == 'train':\n",
    "\n",
    "            max_starting_frame = len(video) - max_frames\n",
    "            try:\n",
    "                starting_frame = random.randint(0, max_starting_frame)\n",
    "            except ValueError:\n",
    "                print(f\"Problem reading {idx} -> {videoname}\")\n",
    "                raise Exception()\n",
    "        else:\n",
    "            raise ValueError(f\"train_or_test must be 'train' or 'test', not {self.train_or_test}\")\n",
    "\n",
    "        video = video[starting_frame:starting_frame + max_frames:frame_incrementer]\n",
    "\n",
    "        label_name = os.path.basename(os.path.dirname(videoname))\n",
    "        label_id = self.classes.index(label_name)\n",
    "        if self.label_per_frame:\n",
    "            label_id = label_id * len(video)  # Label for each frame\n",
    "\n",
    "        if self.transforms:\n",
    "            if self.framewise_transforms:\n",
    "                seed = random.randint(0, 99999)\n",
    "                video_aug = []\n",
    "                for frame in video:\n",
    "                    random.seed(seed)\n",
    "                    video_aug.append(self.transforms(image=frame)['image'])\n",
    "                video_aug = np.array(video_aug)\n",
    "                video = video_aug\n",
    "            else:\n",
    "                video = self.transforms(video)\n",
    "\n",
    "        if type(video) == list:  # Transforms may return a list\n",
    "            video = np.array(video)\n",
    "\n",
    "        x = torch.from_numpy(video.transpose([3, 0, 1, 2])).float()\n",
    "\n",
    "        if self.i3d_norm:\n",
    "            x = (x / 255.) * 2 - 1\n",
    "\n",
    "        y = torch.tensor(label_id, dtype=torch.float)\n",
    "        if self.bce_labels:  # BCEloss expects batch*size * 1 shape, not just batch_size\n",
    "            y = y.unsqueeze(-1)\n",
    "        else:\n",
    "            y = y.long()\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T03:13:16.952337Z",
     "start_time": "2020-09-19T03:13:16.948340Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_incrementer = random.randint(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T03:13:17.315119Z",
     "start_time": "2020-09-19T03:13:17.309706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_incrementer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mtcnn': conda)",
   "language": "python",
   "name": "python37664bitmtcnnconda9d1b84ed94274c82bf8353d472d37ed4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
